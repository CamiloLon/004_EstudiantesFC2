#include "ml_public_interface.h"

#include <iostream>
#include <iomanip>
#include <Eigen/Core>
#include <vector>
#include <array>
#include <string>
#include <optional>

// the main() function that organizes all function calls: the reading of the datasets generated by the Ising part, setting the parameters defining different networks, training them, determining the best one, using the best one to get the outputs for the continuous temperature dataset and writing the corresponding files

int main() {
    
    std::cout << "\nSTATUS\t\t\t\t\t TIMESTAMP" << "\n__________________________________________________\n" << std::endl;
    std::cout << "\nStarting compilation of ML main.\t" << time_now << std::endl;

    // optional container for the best neural network, will be updated whenever a configuration reaches a better final training accuracy than the one before
    std::optional<NeuralNetwork> current_best;
    
    /* TRAINING DIFFERENT NEURAL NETWORKS TO FIND THE BEST */
    {
        std::cout << "\n\nReading in training/validation datasets." << time_now << std::endl;

        // load training data (struct)
        const InputData training = interpret_inputs_and_targets_from_binary(
            "training_inputs_bin"
        );

        // create matrices that hold inputs and targets of training data (loaded previously as a struct) and parameters for later usage
        const Eigen::MatrixXd training_inputs{training.inputs};
        const Eigen::MatrixXd training_targets{training.targets};
        const Index input_vector_size{training_inputs.rows()};
        const Index training_set_size{training_inputs.cols()};
        const Index target_vector_size{training_targets.rows()};

        // load validation data (struct)
        const InputData validation = interpret_inputs_and_targets_from_binary(
            "validation_inputs_bin"
        );

        // create matrices that hold inputs and targets of validation data (loaded previously as a struct) and parameters for later usage
        const Eigen::MatrixXd validation_inputs{validation.inputs};
        const Eigen::MatrixXd validation_targets{validation.targets};
        const Index validation_set_size{validation_inputs.cols()};

        std::cout << "...complete.\t\t\t\t" << time_now << std::endl;
        std::cout << "\nTraining all configurations.\t\t" << time_now << std::endl;

        // define system parameters that will be used for all systems. Will be checked for viability in the constructor body of NeuralNetwork. 
        const int epochs{10};
        const Index batch_size{100};
        const bool run_validation_during_training{true};
        const Index batches_per_validation{5};

        // define fixed system parameters that will be used (name, number of layers, amount of neurons for each layer)
        // names
        const std::vector<std::string> system_names {
            "system_config_1",
            "system_config_2"
        };

        // layers (not counting the input layer)
        const std::vector<size_t> amounts_of_layers {
            2,
            3
        };

        // neurons per layer
        const std::vector<std::vector<Index>> neurons_per_layer { // (in the case of ising, input_vector_size = 100 and target_vector_size = 2)
            {input_vector_size, 30, target_vector_size},
            {input_vector_size, 50, 25, target_vector_size}
        };
        
        // define the different learning rates that will be tested for each otherwise fixed system
        const std::vector<double> learning_rates_per_system{.01, .1, 1, 10};

        // for every system: instanciate it, pass input data and validation data, train it (validation data will be written to .txt meanwhile), validate it. If the final accuracy is better than the best so far, then update current_best with this system's parameters.
        for (size_t i{0}; i < system_names.size(); ++i) {
            std::cout << "..." << system_names[i] << "\t\t\t" << time_now << std::endl;
            // loop over all learning rates used for each system
            for (size_t j{0}; j < learning_rates_per_system.size(); ++j) {
                // construct neural network object
                NeuralNetwork neural(
                    system_names[i],
                    amounts_of_layers[i],
                    neurons_per_layer[i]
                );
                // load training data into the network
                neural.LoadTrainingData(
                    training_set_size,
                    training_inputs,
                    training_targets
                );
                // load validation data into the network
                neural.LoadValidationData(
                    validation_set_size,
                    validation_inputs,
                    validation_targets
                );
                // train the network (current learning rate determined by the loop)
                neural.Train(
                    epochs,
                    batch_size,
                    learning_rates_per_system[j],
                    run_validation_during_training,
                    batches_per_validation
                );
                // update current_best with this system's parameters if the current system's total accuracy is better than any before (or if it's the first one)
                if (!current_best || neural.get_final_validation_results_().p_total > current_best->get_final_validation_results_().p_total) {
                    current_best.emplace(neural); // declaring  std::optional<NeuralNetwork> current_best;  and using .emplace() is the only way to "copy" an object of type NeuralNetwork (because it has const member variables).
                }
                std::cout << "   ...completed eta = " << std::fixed << std::setprecision(2) << learning_rates_per_system[j] << "\t\t" << time_now << std::endl;
            }
        }
    }
    NeuralNetwork best_neural = *current_best;
    std::cout << "...complete.\t\t\t\t" << time_now << std::endl;


    /* RUN CONTINUOUS DATASET ONLY ON THE NETWORK WITH THE BEST FINAL ACCURACY */

    /*
    This scope is incomplete. In its final form, the commented functions would be implemented. EvaluateUnlabeledData() would return the outputs of the network for the continuous_inputs argument, and write_continuous_outputs_to_txt would write these outputs to a .txt file, so that the temperature of maximum confusion can be deducted from the point where the outputs of the output layer neurons intersect. These functions would also have corresponding argument- and internal-state-checking functions added to the ErrorManager class.
    */
    {    
        std::cout << "\n\nReading in continuous dataset.\t\t" << time_now << std::endl;
            
        // load continuous data (struct)
        const ContinuousData continuous = interpret_continuous_from_binary(
            "continuous_inputs_bin"
        );

        // create matrix that holds the inputs of the continuous data, a vector that holds the T range and a parameter for later usage
        const Eigen::MatrixXd continuous_inputs = continuous.inputs;
        const Eigen::VectorXd T_range = continuous.temperatures;
        [[maybe_unused]] const int vectors_per_temperature = continuous.vectors_per_temperature;

        // calculate outputs for the continuous input data
        // Eigen::MatrixXd continuous_outputs = best_neural.EvaluateUnlabeledData(
        //     continuous_set_size,
        //     continuous_inputs
        // );

        std::cout << "...outputs generated\t\t\t" << time_now << std::endl;

        // write outputs to .txt
        // write_continuous_outputs_to_txt(
        //     continuous_outputs,
        //     T_range,
        //     vectors_per_temperature
        // );

        std::cout << "...results written to .txt\t\t" << time_now << std::endl;
    }
    
    std::cout << "\n\nCompilation of main successful.\t\t" << time_now << "\n" << std::endl;
    return 0;

}